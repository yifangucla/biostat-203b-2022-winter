---
title: "Biostat 203B Homework 4"
subtitle: Due Mar 18 @ 11:59PM
author: Yi Fang
output:
  html_document:
    toc: true
    toc_depth: 4
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(miceRanger))
```

## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 

1. Explain the jargon MCAR, MAR, and MNAR.

If the probability of being missing is the same for all cases, then the data are said to be missing completely at random (MCAR). 
If the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR).
If neither MCAR nor MAR holds, then we speak of missing not at random (MNAR). MNAR means that the probability of being missing varies for reasons that are unknown to us. 

2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

Multiple Imputation by Chained Equations is a robust, informative method of dealing with missing data in datasets. The procedure ‘fills in’ (imputes) missing data in a dataset through an iterative series of predictive models. In each iteration, each specified variable in the dataset is imputed using the other variables in the dataset. These iterations should be run until it appears that convergence has been met.

3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

First, I manually check which data have substantial missingness, and I find that `marital status` and `calcium` are 2 variables with more than 4000 missing value, so I only keep the variables that are related.
Then, I check the remainning variables, and I do not find any apparent data entry errors, so I do not replace apparent data entry errors by `NA`s.

```{r}
icu_cohort <- readRDS("~/homework-203B-2022-Winter/hw3/mimiciv_shiny/icu_cohort.rds")
colSums(is.na(icu_cohort))
```


4. Impute missing values by `miceRanger` (request $m=3$ data sets). This step is computational intensive. Make sure to save the imputation results as a file. Hint: Setting `max.depth=10` in the `miceRanger` function may cut some computing time.

```{r}
# icu_cohort <- icu_cohort %>%
#   select(ethnicity,
#   language, 
#   insurance,
#   gender,
#   first_careunit,
#   anchor_age,
#   creatinine,
#   potassium,
#   sodium,
#   chloride,
#   bicarbonate,
#   hematocrit,
#   white_blood_cell_count,
#   glucose,
#   magnesium,
#   heart_rate,
#   mean_non_invasive_blood_pressure,
#   systolic_non_invasive_blood_pressure,
#   body_temperature_in_Fahrenheit,
#   respiratory_rate,
#   thirty_day)
# 
# mice <- miceRanger(
#    icu_cohort,
#    m=3,
#    returnModels = FALSE,
#    verbose = FALSE,
#    max.depth=10)
# 
# write_rds(mice, "~/homework-203B-2022-Winter/hw4/mice.rds")
```

5. Make imputation diagnostic plots and explain what they mean.
```{r}
mice <- readRDS("~/homework-203B-2022-Winter/hw4/mice.rds")
plotDistributions(mice)
plotCorrelations(mice)
plotModelError(mice)
```

The distribution plot of imputed values are created first. The red lines are the original data, and the 3 black lines are the imputed values after the MICE. We can find that in almost all variables, the data matches pretty well.
The second plot is the correlation plot. This plot shows a boxplot of the correlations between imputed values in every combination of datasets, at each iteration.
Finally, I plot a OOB error plot. Each model returns the OOB accuracy for classification, and r-squared for regression. We can find that the accuracy for most of the variable are not that good.

6. Choose one of the imputed data sets to be used in Q2. This is **not** a good idea to use just one imputed data set or to average multiple imputed data sets. Explain in a couple of sentences what the correct Multiple Imputation strategy is.

I choose to use the first imputed data set in Q2. 
We do not want to use a single data set, because a single data set may not be perfect or accurate. We want to use different imputed data sets, and analysis all the data sets, then integrate the result of all the data sets. I will use only one data set in this homework for the time reason.


## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function in base R or keras), (2) logistic regression with lasso penalty (glmnet or keras package), (3) random forest (randomForest package), or (4) neural network (keras package).

First, select the first MICE from the 3 MICE result.
```{r}
data <- completeData(mice)
data <- data[[1]]
data
```

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

```{r}
library(caret)
set.seed(3456)
trainIndex <- createDataPartition(data$thirty_day, p=0.8, list=FALSE)
data_train <- data[trainIndex,]
data_test <- data[-trainIndex,]
```

(1) Logistic regression.

2. Train the models using the training set.

```{r}
log_model <- glm(thirty_day~., data = data_train, family = 'binomial')
summary(log_model)
```

3. Compare model prediction performance on the test set.

```{r}
prob_log <- predict(log_model, data_test, type = "response")
predict_log <- ifelse(prob_log > 0.5, TRUE, FALSE)
table(observed = data_test$thirty_day, predicted = predict_log)
cat("Test Accuracy", mean(predict_log == data_test$thirty_day))
```

(2) Logistic regression with lasso penalty using glmnet package.

2. Train the models using the training set.

```{r}
library(glmnet)
cv.out <- cv.glmnet(model.matrix(thirty_day ~ ., data_train), 
                    data_train$thirty_day,
                    alpha = 1, family = "binomial", type.measure = "mse")
cv.out
lasso_min <- cv.out$lambda.min
lasso_lse <- cv.out$lambda.1se
coef(cv.out, s = lasso_lse)
```

3. Compare model prediction performance on the test set.

```{r}
prob_lasso <- 
  predict(cv.out, 
          newx = model.matrix(data_test$thirty_day ~ ., data_test),
          s = lasso_lse, type = "response")
predict_lasso <- ifelse(prob_lasso > 0.5, TRUE, FALSE)
table(observed = data_test$thirty_day, predicted = predict_lasso)
cat("Test Accuracy", mean(predict_lasso == data_test$thirty_day))
```

As the result shows, the logistic regression method has an accuracy of 90.05%, and logistic regression with lasso penalty has a result of 89.99%. Numerically, the logistic regression method is better. However, the two result are basically the same, based on the data error.

